{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'anaconda3 (Python 3.9.12)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# importing the required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "import pymc3 as pm\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opeing the csv file from url link\n",
    "\n",
    "df = pd.read_csv(r'https://raw.githubusercontent.com/susanli2016/Machine-Learning-with-Python/master/data/renfe_small.csv')\n",
    "df= df.sample(frac=0.01, random_state=99)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open various file types; as variation\n",
    "\n",
    "# opening a zip file\n",
    "# with zipfile.ZipFile(\"C:\\\\Users\\\\HERLAB26\\\\Downloads\\\\Data_file.zip\", \"r\",) as zip_ref:\n",
    "#     '''extracting file to a directory set temporarily'''\n",
    "#     zip_ref.extractall(\"temp\")\n",
    "# # print()\n",
    "\n",
    "# # Reading a csvfile using pandas\n",
    "# df1 = pd.read_csv(\"C:\\\\Users\\\\HERLAB26\\\\Downloads\\\\Data_file 2\\\\list.csv\")\n",
    "\n",
    "# df1.head()\n",
    "\n",
    "# Reading a text document\n",
    "# df2 = pd.read_fwf('C:\\\\Users\\\\HERLAB26\\\\Downloads\\\\Data_file 3\\\\words_file.txt', sep=\" \", header=None, names=['col1','col2'])\n",
    "# df2.head\n",
    "# df2.fillna('--') # Filling the NAN values with (--) as a data cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_kde(data['price'].values, rug=True)\n",
    "plt.yticks([0], alpha=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_g:\n",
    "    μ = pm.Uniform('μ', lower=0, upper=300)\n",
    "    σ = pm.HalfNormal('σ', sd=10)\n",
    "    y = pm.Normal('y', mu=μ, sd=σ, observed=data['price'].values)\n",
    "    trace_g = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base model\n",
    "eps = 1e-3\n",
    "\n",
    "def dirichlet_expectation(alpha):\n",
    "    if len(alpha.shape) == 1:\n",
    "        return psi(alpha) - psi(np.sum(alpha))\n",
    "    return psi(alpha) - psi(np.sum(alpha, 1))[:, np.newaxis]\n",
    "\n",
    "class dmafp:\n",
    "    def _init_(self, a_doc, a_voca, a_topic, alpha = 0.1, beta = 0.01, is_compute_bound=True):\n",
    "        self.a_voca = a_voca\n",
    "        self.a_topic = a_topic\n",
    "        self.a_doc = a_doc\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta \n",
    "\n",
    "        self._lambda = 1 * np.random.gamma(100., 1. / 100, (self.a_topic, self.a_voca))\n",
    "        self._Elogbeta = dirichlet_expectation(self._lambda)\n",
    "        self.expElogbeta = np.exp(self._Elogbeta)\n",
    "        self.gamma_iter = 5\n",
    "        self.gamma = 1 * np.random.gamma(100., 1. / 100, (self.a_topic, self.a_voca))\n",
    "        \n",
    "        self.is_compute_bound = is_compute_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase modelling, for strings\n",
    "\n",
    "import gensim\n",
    "\n",
    "# Bigram and Trigram modelling\n",
    "bigram = gensim.models.Phrases(df, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[df], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# checking the posterior summary of the parameters\n",
    "\n",
    "az.summary(trace_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into train and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, :-1]\n",
    "\n",
    "data_train = X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.8,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "# Printing the train and test sets\n",
    "print('X_train : ')\n",
    "print(X_train.head())\n",
    "\n",
    "print('X_test : ')\n",
    "print(X_test.head())\n",
    "\n",
    "print('y_train : ')\n",
    "print(y_train.head())\n",
    "\n",
    "print('y_test : ')\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the gaussian model trace\n",
    "\n",
    "az.plot(trace_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian inference results\n",
    "\n",
    "az.plot_posterior(trace_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting joint distribution of the parameters\n",
    "\n",
    "az.plot_joint(trace_g, kind='kde', fill_last=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIERARCHICAL MODEL\n",
    "\n",
    "train_type_names = df.train_type.unique()\n",
    "train_type_idx = df.train_type_encode.values\n",
    "\n",
    "n_train_types = len(df.train_type.unique())\n",
    "\n",
    "df[['train_type', 'price', 'fare_encode']].head()\n",
    "\n",
    "with pm.Model() as hierarchical_model:\n",
    "    '''global model parameters'''\n",
    "    α_μ_tmp = pm.Normal('α_μ_tmp', mu=0., sd=100)\n",
    "    α_σ_tmp = pm.HalfNormal('α_σ_tmp', 5.)\n",
    "    β_μ = pm.Normal('β_μ', mu=0., sd=100)\n",
    "    β_σ = pm.HalfNormal('β_σ', 5.)\n",
    "\n",
    "    '''train type specific model parameters'''\n",
    "    α_tmp = pm.Normal('α_tmp', mu=α_μ_tmp, sd=α_σ_tmp, shape=n_train_types)  \n",
    "    # Intercept for each train type, distributed around train type mean \n",
    "    β = pm.Normal('β', mu=β_μ, sd=β_σ, shape=n_train_types)\n",
    "    '''Model error'''\n",
    "    eps = pm.HalfCauchy('eps', 5.)\n",
    "\n",
    "    fare_est = α_tmp[train_type_idx] + β[train_type_idx]*df.fare_encode.values\n",
    "\n",
    "    '''Data likelihood'''\n",
    "    fare_like = pm.Normal('fare_like', mu=fare_est, sd=eps, observed=df.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model Summary in hirarchy\n",
    "\n",
    "with hierarchical_model:\n",
    "    hierarchical_trace = pm.sample(2000, tune=2000, target_accept=0.9)\n",
    "\n",
    "pm.traceplot(hierarchical_trace, var_names=['alpha_tmp'],coords={'alpha_tmp_dim_0': range(5)});"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "994423a8c3c15c2a906bbbdb78230761ebe338b645e81cc805c02c60ecb29643"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
